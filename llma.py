# -*- coding: utf-8 -*-
"""LLMa.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zHoSRYp9myCQF8jsKicHtBTvYQSjkZPg
"""

!pip install -q bitsandbytes datasets accelerate loralib
!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git

!pip install  peft

from huggingface_hub import notebook_login
notebook_login()

from typing import Dict, List
from datasets import Dataset, load_dataset, disable_caching
disable_caching()
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
import torch
from torch.utils.data import Dataset
from IPython.display import Markdown

dataset = load_dataset("adas014/kullan" , split = 'train')
small_dataset = dataset.select([i for i in range(260)])
print(small_dataset)
print(small_dataset[0])

prompt_template = """<|begin_of_text|> İlaç etkileşimlerini değerlendirerek, potansiyel sağlık risklerini açıklayın. {soru} \n """
answer_template = """{uzun_cevap}"""



def _add_text(rec):
    instruction = rec["soru"]

    response = rec["uzun_cevap"]


    if not instruction:
        raise ValueError(f"Expected an instruction in: {rec}")
    if not response:
        raise ValueError(f"Expected a response in: {rec}")


    rec["prompt"] = prompt_template.format(soru=instruction)
    rec["answer"] = answer_template.format(uzun_cevap=response)


    rec["text"] = rec["prompt"] + rec["answer"]
    return rec


small_dataset = small_dataset.map(_add_text)


print(small_dataset[0])

generate__dataset = small_dataset.map(_add_text)
print(generate__dataset[0])

print(generate__dataset['text'][5])

model_id = "ytu-ce-cosmos/Turkish-Llama-8b-Instruct-v0.1"
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token = tokenizer.eos_token


model = AutoModelForCausalLM.from_pretrained(
    model_id,
    # use_cache=False,
    device_map="auto",
    load_in_8bit=True,
    torch_dtype=torch.float16
)


model.resize_token_embeddings(len(tokenizer))

from functools import partial
import copy
from transformers import DataCollatorForSeq2Seq
from typing import Dict, List

def _preprocess_batch(batch: Dict[str, List], tokenizer):

    model_inputs = tokenizer(batch["text"], truncation=True, padding='max_length', max_length=512)


    model_inputs["labels"] = copy.deepcopy(model_inputs['input_ids'])

    return model_inputs

_preprocessing_function = partial(_preprocess_batch, tokenizer=tokenizer)

encoded_small_dataset = generate__dataset.map(
    _preprocessing_function,
    batched=True,
    remove_columns=["soru", "uzun_cevap","prompt","answer"],
)


processed_dataset = encoded_small_dataset.filter(lambda rec: len(rec["input_ids"]) <= tokenizer.model_max_length)


split_dataset = processed_dataset.train_test_split(test_size=0.10, seed=0)
print(split_dataset)


data_collator = DataCollatorForSeq2Seq(
    model=model, tokenizer=tokenizer, padding='max_length', pad_to_multiple_of=8
)

from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

LORA_R = 64
LORA_ALPHA = 128
LORA_DROPOUT = 0.05

lora_config = LoraConfig(
                 r = LORA_R,
                 lora_alpha = LORA_ALPHA,
                 lora_dropout = LORA_DROPOUT,
                 bias="none",
                 task_type="CAUSAL_LM",
                 target_modules=["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj","lm_head"],
)


model = prepare_model_for_kbit_training(model)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

from transformers import TrainingArguments, Trainer
import bitsandbytes

training_args = TrainingArguments(
    fp16=True,
    output_dir="./outputss",
    num_train_epochs=1,
    max_steps=40,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    optim="paged_adamw_32bit",
    save_strategy="steps",
    save_steps=20,
    learning_rate=2e-4,
    weight_decay=0.001,
    evaluation_strategy="steps",
    eval_steps=20,
    do_eval=True,
    report_to="none",
    logging_dir="./logns",
    logging_steps=20,
    gradient_checkpointing=True,  # Reduce memory usage
    gradient_checkpointing_kwargs={"use_reentrant": True},
)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=training_args,
    train_dataset=split_dataset['train'],
    eval_dataset=split_dataset["test"],
    data_collator=data_collator,
)

model.config.use_cache = False
trainer.train()

from huggingface_hub import HfApi

repository_id = "LLamasonverone"


trainer.model.push_to_hub(repository_id)

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, PeftConfig

peft_model_id = "adas014/LLamasonverone"
config = PeftConfig.from_pretrained(peft_model_id)
model = AutoModelForCausalLM.from_pretrained(
    config.base_model_name_or_path,
    return_dict = True,
    load_in_4bit = True,
    #load_in_8bit_fp32_cpu_offload=True,
    device_map = "auto"
)
tokenizer=AutoTokenizer.from_pretrained(config.base_model_name_or_path)
model=PeftModel.from_pretrained(model,peft_model_id)

print(tokenizer.special_tokens_map)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Soru metni
soru = "İlaç etkileşimlerini değerlendirerek, potansiyel kronik hastalık risklerini açıklayın. depresyon hastasıyım miğren ilacı ile birlikte kullanımı hangi kronik hastalıklara neden olur? "

# Default prompt oluşturma
default_prompt = f""" <|begin_of_text|>  {soru}  \n"""

# Prompt'u token'lara dönüştürme
batch = tokenizer(default_prompt, return_tensors='pt').to(device)

terminators = [
    tokenizer.eos_token_id,
    tokenizer.convert_tokens_to_ids("<|eot_id|>")
]
# Modelden cevap üretme
output_tokens = model.generate(
    **batch,
    max_new_tokens=256,   # Yeni token sayısını belirler
    num_return_sequences=1,  # Bir tane cevap döndür
    temperature=0.1,  # Çıktı çeşitliliği (0 ile 1 arasında ayarlanabilir)
    top_p=0.95,  # Nucleus sampling
   top_k=50,  # Top-k sampling
    repetition_penalty=1.2,  # Aynı yanıtların tekrarını engelle
    no_repeat_ngram_size=2,  # N-gram tekrarını engelle
  eos_token_id=terminators,  # Cümle sonlandırma token'ı



)




output_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)

print(output_text)

import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

class IlacHastalikEvaluator:
    def __init__(self, model, tokenizer, device):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device

    def generate_prompt(self, ilac: str, istenilen: str):
        default_prompt_ilac = f"<|begin_of_text|> İlaç etkileşimlerini değerlendirerek, potansiyel sağlık risklerini açıklayın.\n {ilac} ve {istenilen} birlikte kullanımı hangi kronik hastalıklara neden olur? \n"
        default_prompt_hastalik = f"<|begin_of_text|> İlaç etkileşimlerini değerlendirerek, potansiyel sağlık risklerini açıklayın.\n {ilac} hastasıyım ve {istenilen} birlikte kullanımı hangi kronik hastalıklara neden olur? \n"
        if self.is_ilac(ilac):
            return default_prompt_ilac
        elif self.is_hastalik(ilac):
            return default_prompt_hastalik
        else:
            return None

    def is_ilac(self, text: str):
        ilaç_listesi = ["aspirin", "parol", "klopidogrel", "metformin", "ağrı kesici", "grip ilacı", "kan sulandırıcı", "antibiyotik", "depresyon", "potasyum", "miğren"]
        return any(ilac.lower() in text.lower() for ilac in ilaç_listesi)

    def is_hastalik(self, text: str):
        hastalik_listesi = ["hipertansiyon", "diyabet", "kalp hastalığı", "astım", "şeker", "tansiyon", "depresyon"]
        return any(hastalik.lower() in text.lower() for hastalik in hastalik_listesi)

    def get_response(self, ilac: str, istenilen: str):
        prompt = self.generate_prompt(ilac, istenilen)

        if prompt:
            batch = self.tokenizer(prompt, return_tensors='pt').to(self.device)
            output_tokens = self.model.generate(**batch, max_new_tokens=512,eos_token_id=tokenizer.eos_token_id)
            generated_answer = self.tokenizer.decode(output_tokens[0], skip_special_tokens=True)
            # Sadece model cevabını döndürmek için prompt kısmını temizliyoruz
            return generated_answer.strip()
        else:
            return "Geçersiz giriş. Lütfen geçerli bir ilaç veya hastalık girin."

# Kullanıcı girişi ve model çıktısı
evaluator = IlacHastalikEvaluator(model, tokenizer, device)

ilac = input("Kullandığınız ilaç veya hastalığınızı giriniz: ")
istenilen = input("Kullanmak istediğiniz ilacı giriniz: ")

cevap = evaluator.get_response(ilac, istenilen)
print(cevap)

import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

class IlacHastalikEvaluator:
    def __init__(self, model, tokenizer, device):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device

    def generate_prompt(self, ilac: str, istenilen: str):
        default_prompt_ilac = f"<|begin_of_text|> İlaç etkileşimlerini değerlendirerek, potansiyel sağlık risklerini açıklayın.\n {ilac} ve {istenilen} birlikte kullanımı hangi kronik hastalıklara neden olur? \n"
        default_prompt_hastalik = f"<|begin_of_text|> İlaç etkileşimlerini değerlendirerek, potansiyel sağlık risklerini açıklayın.\n {ilac} hastasıyım ve {istenilen} birlikte kullanımı hangi kronik hastalıklara neden olur? \n"
        if self.is_ilac(ilac):
            return default_prompt_ilac
        elif self.is_hastalik(ilac):
            return default_prompt_hastalik
        else:
            return None

    def is_ilac(self, text: str):
        ilaç_listesi = ["aspirin", "parol", "klopidogrel", "metformin", "ağrı kesici", "grip ilacı", "kan sulandırıcı", "antibiyotik", "depresyon", "potasyum", "miğren"]
        return any(ilac.lower() in text.lower() for ilac in ilaç_listesi)

    def is_hastalik(self, text: str):
        hastalik_listesi = ["hipertansiyon", "diyabet", "kalp", "astım", "şeker", "tansiyon", "depresyon"]
        return any(hastalik.lower() in text.lower() for hastalik in hastalik_listesi)

    def get_response(self, ilac: str, istenilen: str):
        prompt = self.generate_prompt(ilac, istenilen)

        if prompt:
            batch = self.tokenizer(prompt, return_tensors='pt').to(self.device)
            output_tokens = self.model.generate(**batch, max_new_tokens=512,eos_token_id=tokenizer.eos_token_id)
            generated_answer = self.tokenizer.decode(output_tokens[0], skip_special_tokens=True)
            # Sadece model cevabını döndürmek için prompt kısmını temizliyoruz
            return generated_answer.strip()
        else:
            return "Geçersiz giriş. Lütfen geçerli bir ilaç veya hastalık girin."

# Kullanıcı girişi ve model çıktısı
evaluator = IlacHastalikEvaluator(model, tokenizer, device)

ilac = input("Kullandığınız ilaç veya hastalığınızı giriniz: ")
istenilen = input("Kullanmak istediğiniz ilacı giriniz: ")

cevap = evaluator.get_response(ilac, istenilen)
print(cevap)

import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

class IlacHastalikEvaluator:
    def __init__(self, model, tokenizer, device):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device

    def generate_prompt(self, ilac: str, istenilen: str):
        default_prompt_ilac = f"<|begin_of_text|> İlaç etkileşimlerini değerlendirerek, potansiyel sağlık risklerini açıklayın.\n {ilac} ve {istenilen} birlikte kullanımı hangi kronik hastalıklara neden olur? \n"
        default_prompt_hastalik = f"<|begin_of_text|> İlaç etkileşimlerini değerlendirerek, potansiyel sağlık risklerini açıklayın.\n {ilac} hastasıyım ve {istenilen} birlikte kullanımı hangi kronik hastalıklara neden olur? \n"
        if self.is_ilac(ilac):
            return default_prompt_ilac
        elif self.is_hastalik(ilac):
            return default_prompt_hastalik
        else:
            return None

    def is_ilac(self, text: str):
        ilaç_listesi = ["aspirin", "parol", "klopidogrel", "metformin", "ağrı kesici", "grip ilacı", "kan sulandırıcı", "antibiyotik", "depresyon", "potasyum", "miğren"]
        return any(ilac.lower() in text.lower() for ilac in ilaç_listesi)

    def is_hastalik(self, text: str):
        hastalik_listesi = ["hipertansiyon", "diyabet", "kalp", "astım", "şeker", "tansiyon", "depresyon"]
        return any(hastalik.lower() in text.lower() for hastalik in hastalik_listesi)

    def get_response(self, ilac: str, istenilen: str):
        prompt = self.generate_prompt(ilac, istenilen)

        if prompt:
            batch = self.tokenizer(prompt, return_tensors='pt').to(self.device)
            output_tokens = self.model.generate(**batch, max_new_tokens=512,eos_token_id=tokenizer.eos_token_id)
            generated_answer = self.tokenizer.decode(output_tokens[0], skip_special_tokens=True)
            # Sadece model cevabını döndürmek için prompt kısmını temizliyoruz
            return generated_answer.strip()
        else:
            return "Geçersiz giriş. Lütfen geçerli bir ilaç veya hastalık girin."

# Kullanıcı girişi ve model çıktısı
evaluator = IlacHastalikEvaluator(model, tokenizer, device)

ilac = input("Kullandığınız ilaç veya hastalığınızı giriniz: ")
istenilen = input("Kullanmak istediğiniz ilacı giriniz: ")

cevap = evaluator.get_response(ilac, istenilen)
print(cevap)

import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

class IlacHastalikEvaluator:
    def __init__(self, model, tokenizer, device):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device

    def generate_prompt(self, ilac: str, istenilen: str):
        default_prompt_ilac = f"<|begin_of_text|> İlaç etkileşimlerini değerlendirerek, potansiyel sağlık risklerini açıklayın.\n {ilac} ve {istenilen} birlikte kullanımı hangi kronik hastalıklara neden olur? \n"
        default_prompt_hastalik = f"<|begin_of_text|> İlaç etkileşimlerini değerlendirerek, potansiyel sağlık risklerini açıklayın.\n {ilac} hastasıyım ve {istenilen} birlikte kullanımı hangi kronik hastalıklara neden olur? \n"
        if self.is_ilac(ilac):
            return default_prompt_ilac
        elif self.is_hastalik(ilac):
            return default_prompt_hastalik
        else:
            return None

    def is_ilac(self, text: str):
        ilaç_listesi = ["aspirin", "parol", "klopidogrel", "metformin", "ağrı kesici", "grip ilacı", "kan sulandırıcı", "antibiyotik", "depresyon", "potasyum", "miğren"]
        return any(ilac.lower() in text.lower() for ilac in ilaç_listesi)

    def is_hastalik(self, text: str):
        hastalik_listesi = ["hipertansiyon", "diyabet", "kalp", "astım", "şeker", "tansiyon", "depresyon"]
        return any(hastalik.lower() in text.lower() for hastalik in hastalik_listesi)

    def get_response(self, ilac: str, istenilen: str):
        prompt = self.generate_prompt(ilac, istenilen)

        if prompt:
            batch = self.tokenizer(prompt, return_tensors='pt').to(self.device)
            output_tokens = self.model.generate(**batch, max_new_tokens=512,eos_token_id=tokenizer.eos_token_id)
            generated_answer = self.tokenizer.decode(output_tokens[0], skip_special_tokens=True)
            # Sadece model cevabını döndürmek için prompt kısmını temizliyoruz
            return generated_answer.strip()
        else:
            return "Geçersiz giriş. Lütfen geçerli bir ilaç veya hastalık girin."

# Kullanıcı girişi ve model çıktısı
evaluator = IlacHastalikEvaluator(model, tokenizer, device)

ilac = input("Kullandığınız ilaç veya hastalığınızı giriniz: ")
istenilen = input("Kullanmak istediğiniz ilacı giriniz: ")

cevap = evaluator.get_response(ilac, istenilen)
print(cevap)

import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

class IlacHastalikEvaluator:
    def __init__(self, model, tokenizer, device):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device

    def generate_prompt(self, ilac: str, istenilen: str):
        default_prompt_ilac = f"<|begin_of_text|> İlaç etkileşimlerini değerlendirerek, potansiyel sağlık risklerini açıklayın.\n {ilac} ve {istenilen} birlikte kullanımı hangi kronik hastalıklara neden olur? \n"
        default_prompt_hastalik = f"<|begin_of_text|> İlaç etkileşimlerini değerlendirerek, potansiyel sağlık risklerini açıklayın.\n {ilac} hastasıyım ve {istenilen} birlikte kullanımı hangi kronik hastalıklara neden olur? \n"
        if self.is_ilac(ilac):
            return default_prompt_ilac
        elif self.is_hastalik(ilac):
            return default_prompt_hastalik
        else:
            return None

    def is_ilac(self, text: str):
        ilaç_listesi = ["aspirin", "parol", "klopidogrel", "metformin", "ağrı kesici", "grip ilacı", "kan sulandırıcı", "antibiyotik", "depresyon", "potasyum", "miğren"]
        return any(ilac.lower() in text.lower() for ilac in ilaç_listesi)

    def is_hastalik(self, text: str):
        hastalik_listesi = ["hipertansiyon", "diyabet", "kalp", "astım", "şeker", "tansiyon", "depresyon"]
        return any(hastalik.lower() in text.lower() for hastalik in hastalik_listesi)

    def get_response(self, ilac: str, istenilen: str):
        prompt = self.generate_prompt(ilac, istenilen)

        if prompt:
            batch = self.tokenizer(prompt, return_tensors='pt').to(self.device)
            output_tokens = self.model.generate(**batch, max_new_tokens=512,eos_token_id=tokenizer.eos_token_id)
            generated_answer = self.tokenizer.decode(output_tokens[0], skip_special_tokens=True)
            # Sadece model cevabını döndürmek için prompt kısmını temizliyoruz
            return generated_answer.strip()
        else:
            return "Geçersiz giriş. Lütfen geçerli bir ilaç veya hastalık girin."

# Kullanıcı girişi ve model çıktısı
evaluator = IlacHastalikEvaluator(model, tokenizer, device)

ilac = input("Kullandığınız ilaç veya hastalığınızı giriniz: ")
istenilen = input("Kullanmak istediğiniz ilacı giriniz: ")

cevap = evaluator.get_response(ilac, istenilen)
print(cevap)

import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

class IlacHastalikEvaluator:
    def __init__(self, model, tokenizer, device):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device

    def generate_prompt(self, ilac: str, istenilen: str):
        default_prompt_ilac = f"<|begin_of_text|> İlaç etkileşimlerini değerlendirerek, potansiyel sağlık risklerini açıklayın.\n {ilac} ve {istenilen} birlikte kullanımı hangi kronik hastalıklara neden olur? \n"
        default_prompt_hastalik = f"<|begin_of_text|> İlaç etkileşimlerini değerlendirerek, potansiyel sağlık risklerini açıklayın.\n {ilac} hastasıyım ve {istenilen} birlikte kullanımı hangi kronik hastalıklara neden olur? \n"
        if self.is_ilac(ilac):
            return default_prompt_ilac
        elif self.is_hastalik(ilac):
            return default_prompt_hastalik
        else:
            return None

    def is_ilac(self, text: str):
        ilaç_listesi = ["aspirin", "parol", "klopidogrel", "metformin", "ağrı kesici", "grip ilacı", "kan sulandırıcı", "antibiyotik", "depresyon", "potasyum", "miğren"]
        return any(ilac.lower() in text.lower() for ilac in ilaç_listesi)

    def is_hastalik(self, text: str):
        hastalik_listesi = ["hipertansiyon", "diyabet", "kalp", "astım", "şeker", "tansiyon", "depresyon"]
        return any(hastalik.lower() in text.lower() for hastalik in hastalik_listesi)

    def get_response(self, ilac: str, istenilen: str):
        prompt = self.generate_prompt(ilac, istenilen)

        if prompt:
            batch = self.tokenizer(prompt, return_tensors='pt').to(self.device)
            output_tokens = self.model.generate(**batch, max_new_tokens=512,eos_token_id=tokenizer.eos_token_id)
            generated_answer = self.tokenizer.decode(output_tokens[0], skip_special_tokens=True)
            # Sadece model cevabını döndürmek için prompt kısmını temizliyoruz
            return generated_answer.strip()
        else:
            return "Geçersiz giriş. Lütfen geçerli bir ilaç veya hastalık girin."

# Kullanıcı girişi ve model çıktısı
evaluator = IlacHastalikEvaluator(model, tokenizer, device)

ilac = input("Kullandığınız ilaç veya hastalığınızı giriniz: ")
istenilen = input("Kullanmak istediğiniz ilacı giriniz: ")

cevap = evaluator.get_response(ilac, istenilen)
print(cevap)

import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

class IlacHastalikEvaluator:
    def __init__(self, model, tokenizer, device):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device

    def generate_prompt(self, ilac: str, istenilen: str):
        default_prompt_ilac = f"<|begin_of_text|> İlaç etkileşimlerini değerlendirerek, potansiyel sağlık risklerini açıklayın.\n {ilac} ve {istenilen} birlikte kullanımı hangi kronik hastalıklara neden olur? \n"
        default_prompt_hastalik = f"<|begin_of_text|> İlaç etkileşimlerini değerlendirerek, potansiyel sağlık risklerini açıklayın.\n {ilac} hastasıyım ve {istenilen} birlikte kullanımı hangi kronik hastalıklara neden olur? \n"
        if self.is_ilac(ilac):
            return default_prompt_ilac
        elif self.is_hastalik(ilac):
            return default_prompt_hastalik
        else:
            return None

    def is_ilac(self, text: str):
        ilaç_listesi = ["aspirin", "parol", "klopidogrel", "metformin", "ağrı kesici", "grip ilacı", "kan sulandırıcı", "antibiyotik", "depresyon", "potasyum", "miğren","anestezi"]
        return any(ilac.lower() in text.lower() for ilac in ilaç_listesi)

    def is_hastalik(self, text: str):
        hastalik_listesi = ["hipertansiyon", "diyabet", "kalp", "astım", "şeker", "tansiyon", "depresyon"]
        return any(hastalik.lower() in text.lower() for hastalik in hastalik_listesi)

    def get_response(self, ilac: str, istenilen: str):
        prompt = self.generate_prompt(ilac, istenilen)

        if prompt:
            batch = self.tokenizer(prompt, return_tensors='pt').to(self.device)
            output_tokens = self.model.generate(**batch, max_new_tokens=512,eos_token_id=tokenizer.eos_token_id)
            generated_answer = self.tokenizer.decode(output_tokens[0], skip_special_tokens=True)
            # Sadece model cevabını döndürmek için prompt kısmını temizliyoruz
            return generated_answer.strip()
        else:
            return "Geçersiz giriş. Lütfen geçerli bir ilaç veya hastalık girin."

# Kullanıcı girişi ve model çıktısı
evaluator = IlacHastalikEvaluator(model, tokenizer, device)

ilac = input("Kullandığınız ilaç veya hastalığınızı giriniz: ")
istenilen = input("Kullanmak istediğiniz ilacı giriniz: ")

cevap = evaluator.get_response(ilac, istenilen)
print(cevap)

import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

class IlacHastalikEvaluator:
    def __init__(self, model, tokenizer, device):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device

    def generate_prompt(self, ilac: str, istenilen: str):
        default_prompt_ilac = f"<|begin_of_text|> İlaç etkileşimlerini değerlendirerek, potansiyel sağlık risklerini açıklayın.\n {ilac} ve {istenilen} birlikte kullanımı hangi kronik hastalıklara neden olur? \n"
        default_prompt_hastalik = f"<|begin_of_text|> İlaç etkileşimlerini değerlendirerek, potansiyel sağlık risklerini açıklayın.\n {ilac} hastasıyım ve {istenilen} birlikte kullanımı hangi kronik hastalıklara neden olur? \n"
        if self.is_ilac(ilac):
            return default_prompt_ilac
        elif self.is_hastalik(ilac):
            return default_prompt_hastalik
        else:
            return None

    def is_ilac(self, text: str):
        ilaç_listesi = ["aspirin", "parol", "klopidogrel", "metformin", "ağrı kesici", "grip ilacı", "kan sulandırıcı", "antibiyotik", "depresyon", "potasyum", "miğren","anestezi","warfarin"]
        return any(ilac.lower() in text.lower() for ilac in ilaç_listesi)

    def is_hastalik(self, text: str):
        hastalik_listesi = ["hipertansiyon", "diyabet", "kalp", "astım", "şeker", "tansiyon", "depresyon"]
        return any(hastalik.lower() in text.lower() for hastalik in hastalik_listesi)

    def get_response(self, ilac: str, istenilen: str):
        prompt = self.generate_prompt(ilac, istenilen)

        if prompt:
            batch = self.tokenizer(prompt, return_tensors='pt').to(self.device)
            output_tokens = self.model.generate(**batch, max_new_tokens=512,eos_token_id=tokenizer.eos_token_id)
            generated_answer = self.tokenizer.decode(output_tokens[0], skip_special_tokens=True)
            # Sadece model cevabını döndürmek için prompt kısmını temizliyoruz
            return generated_answer.strip()
        else:
            return "Geçersiz giriş. Lütfen geçerli bir ilaç veya hastalık girin."

# Kullanıcı girişi ve model çıktısı
evaluator = IlacHastalikEvaluator(model, tokenizer, device)

ilac = input("Kullandığınız ilaç veya hastalığınızı giriniz: ")
istenilen = input("Kullanmak istediğiniz ilacı giriniz: ")

cevap = evaluator.get_response(ilac, istenilen)
print(cevap)

import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

class IlacHastalikEvaluator:
    def __init__(self, model, tokenizer, device):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device

    def generate_prompt(self, ilac: str, istenilen: str):
        default_prompt_ilac = f"<|begin_of_text|> İlaç etkileşimlerini değerlendirerek, potansiyel sağlık risklerini açıklayın.\n {ilac} ve {istenilen} birlikte kullanımı hangi kronik hastalıklara neden olur? \n"
        default_prompt_hastalik = f"<|begin_of_text|> İlaç etkileşimlerini değerlendirerek, potansiyel sağlık risklerini açıklayın.\n {ilac} hastasıyım ve {istenilen} birlikte kullanımı hangi kronik hastalıklara neden olur? \n"
        if self.is_ilac(ilac):
            return default_prompt_ilac
        elif self.is_hastalik(ilac):
            return default_prompt_hastalik
        else:
            return None

    def is_ilac(self, text: str):
        ilaç_listesi = ["aspirin", "parol", "klopidogrel", "metformin", "ağrı kesici", "grip ilacı", "kan sulandırıcı", "antibiyotik", "depresyon", "potasyum", "miğren","anestezi","sakinleştirici","barbitürat"]
        return any(ilac.lower() in text.lower() for ilac in ilaç_listesi)

    def is_hastalik(self, text: str):
        hastalik_listesi = ["hipertansiyon", "diyabet", "kalp", "astım", "şeker", "tansiyon", "depresyon"]
        return any(hastalik.lower() in text.lower() for hastalik in hastalik_listesi)

    def get_response(self, ilac: str, istenilen: str):
        prompt = self.generate_prompt(ilac, istenilen)

        if prompt:
            batch = self.tokenizer(prompt, return_tensors='pt').to(self.device)
            output_tokens = self.model.generate(**batch, max_new_tokens=512,eos_token_id=tokenizer.eos_token_id)
            generated_answer = self.tokenizer.decode(output_tokens[0], skip_special_tokens=True)
            # Sadece model cevabını döndürmek için prompt kısmını temizliyoruz
            return generated_answer.strip()
        else:
            return "Geçersiz giriş. Lütfen geçerli bir ilaç veya hastalık girin."

# Kullanıcı girişi ve model çıktısı
evaluator = IlacHastalikEvaluator(model, tokenizer, device)

ilac = input("Kullandığınız ilaç veya hastalığınızı giriniz: ")
istenilen = input("Kullanmak istediğiniz ilacı giriniz: ")

cevap = evaluator.get_response(ilac, istenilen)
print(cevap)

import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

class IlacHastalikEvaluator:
    def __init__(self, model, tokenizer, device):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device

    def generate_prompt(self, ilac: str, istenilen: str):
        default_prompt_ilac = f"<|begin_of_text|> İlaç etkileşimlerini değerlendirerek, potansiyel sağlık risklerini açıklayın.\n {ilac} ve {istenilen} birlikte kullanımı hangi kronik hastalıklara neden olur? \n"
        default_prompt_hastalik = f"<|begin_of_text|> İlaç etkileşimlerini değerlendirerek, potansiyel sağlık risklerini açıklayın.\n {ilac} hastasıyım ve {istenilen} birlikte kullanımı hangi kronik hastalıklara neden olur? \n"
        if self.is_ilac(ilac):
            return default_prompt_ilac
        elif self.is_hastalik(ilac):
            return default_prompt_hastalik
        else:
            return None

    def is_ilac(self, text: str):
        ilaç_listesi = ["aspirin", "parol", "klopidogrel", "metformin", "ağrı kesici", "grip ilacı", "kan sulandırıcı", "antibiyotik", "depresyon", "potasyum", "miğren","anestezi","sakinleştirici","barbitürat"]
        return any(ilac.lower() in text.lower() for ilac in ilaç_listesi)

    def is_hastalik(self, text: str):
        hastalik_listesi = ["hipertansiyon", "diyabet", "kalp", "astım", "şeker", "tansiyon", "depresyon","kanser"]
        return any(hastalik.lower() in text.lower() for hastalik in hastalik_listesi)

    def get_response(self, ilac: str, istenilen: str):
        prompt = self.generate_prompt(ilac, istenilen)

        if prompt:
            batch = self.tokenizer(prompt, return_tensors='pt').to(self.device)
            output_tokens = self.model.generate(**batch, max_new_tokens=512,eos_token_id=tokenizer.eos_token_id)
            generated_answer = self.tokenizer.decode(output_tokens[0], skip_special_tokens=True)
            # Sadece model cevabını döndürmek için prompt kısmını temizliyoruz
            return generated_answer.strip()
        else:
            return "Geçersiz giriş. Lütfen geçerli bir ilaç veya hastalık girin."

# Kullanıcı girişi ve model çıktısı
evaluator = IlacHastalikEvaluator(model, tokenizer, device)

ilac = input("Kullandığınız ilaç veya hastalığınızı giriniz: ")
istenilen = input("Kullanmak istediğiniz ilacı giriniz: ")

cevap = evaluator.get_response(ilac, istenilen)
print(cevap)

import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

class IlacHastalikEvaluator:
    def __init__(self, model, tokenizer, device):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device

    def generate_prompt(self, ilac: str, istenilen: str):
        default_prompt_ilac = f"<|begin_of_text|> İlaç etkileşimlerini değerlendirerek, potansiyel sağlık risklerini açıklayın.\n {ilac} ve {istenilen} birlikte kullanımı hangi kronik hastalıklara neden olur? \n"
        default_prompt_hastalik = f"<|begin_of_text|> İlaç etkileşimlerini değerlendirerek, potansiyel sağlık risklerini açıklayın.\n {ilac} hastasıyım ve {istenilen} birlikte kullanımı hangi kronik hastalıklara neden olur? \n"
        if self.is_ilac(ilac):
            return default_prompt_ilac
        elif self.is_hastalik(ilac):
            return default_prompt_hastalik
        else:
            return None

    def is_ilac(self, text: str):
        ilaç_listesi = ["aspirin", "parol", "klopidogrel", "metformin", "ağrı kesici", "grip ilacı", "kan sulandırıcı", "antibiyotik", "depresyon", "potasyum", "miğren","anestezi","sakinleştirici","barbitürat"]
        return any(ilac.lower() in text.lower() for ilac in ilaç_listesi)

    def is_hastalik(self, text: str):
        hastalik_listesi = ["hipertansiyon", "diyabet", "kalp", "astım", "şeker", "tansiyon", "depresyon","kanser"]
        return any(hastalik.lower() in text.lower() for hastalik in hastalik_listesi)

    def get_response(self, ilac: str, istenilen: str):
        prompt = self.generate_prompt(ilac, istenilen)

        if prompt:
            batch = self.tokenizer(prompt, return_tensors='pt').to(self.device)
            output_tokens = self.model.generate(**batch, max_new_tokens=512,eos_token_id=tokenizer.eos_token_id)
            generated_answer = self.tokenizer.decode(output_tokens[0], skip_special_tokens=True)
            # Sadece model cevabını döndürmek için prompt kısmını temizliyoruz
            return generated_answer.strip()
        else:
            return "Geçersiz giriş. Lütfen geçerli bir ilaç veya hastalık girin."

# Kullanıcı girişi ve model çıktısı
evaluator = IlacHastalikEvaluator(model, tokenizer, device)

ilac = input("Kullandığınız ilaç veya hastalığınızı giriniz: ")
istenilen = input("Kullanmak istediğiniz ilacı giriniz: ")

cevap = evaluator.get_response(ilac, istenilen)
print(cevap)

import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

class IlacHastalikEvaluator:
    def __init__(self, model, tokenizer, device):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device

    def generate_prompt(self, ilac: str, istenilen: str):
        default_prompt_ilac = f"<|begin_of_text|> İlaç etkileşimlerini değerlendirerek, potansiyel sağlık risklerini açıklayın.\n {ilac} ve {istenilen} birlikte kullanımı hangi kronik hastalıklara neden olur? \n"
        default_prompt_hastalik = f"<|begin_of_text|> İlaç etkileşimlerini değerlendirerek, potansiyel sağlık risklerini açıklayın.\n {ilac} hastasıyım ve {istenilen} birlikte kullanımı hangi kronik hastalıklara neden olur? \n"
        if self.is_ilac(ilac):
            return default_prompt_ilac
        elif self.is_hastalik(ilac):
            return default_prompt_hastalik
        else:
            return None

    def is_ilac(self, text: str):
        ilaç_listesi = ["aspirin", "parol", "klopidogrel", "metformin", "ağrı kesici", "grip ilacı", "kan sulandırıcı", "antibiyotik", "depresyon", "potasyum", "miğren","anestezi","sakinleştirici","barbitürat","çikolata"]
        return any(ilac.lower() in text.lower() for ilac in ilaç_listesi)

    def is_hastalik(self, text: str):
        hastalik_listesi = ["hipertansiyon", "diyabet", "kalp", "astım", "şeker", "tansiyon", "depresyon","kanser"]
        return any(hastalik.lower() in text.lower() for hastalik in hastalik_listesi)

    def get_response(self, ilac: str, istenilen: str):
        prompt = self.generate_prompt(ilac, istenilen)

        if prompt:
            batch = self.tokenizer(prompt, return_tensors='pt').to(self.device)
            output_tokens = self.model.generate(**batch, max_new_tokens=512,eos_token_id=tokenizer.eos_token_id)
            generated_answer = self.tokenizer.decode(output_tokens[0], skip_special_tokens=True)
            # Sadece model cevabını döndürmek için prompt kısmını temizliyoruz
            return generated_answer.strip()
        else:
            return "Geçersiz giriş. Lütfen geçerli bir ilaç veya hastalık girin."

# Kullanıcı girişi ve model çıktısı
evaluator = IlacHastalikEvaluator(model, tokenizer, device)

ilac = input("Kullandığınız ilaç veya hastalığınızı giriniz: ")
istenilen = input("Kullanmak istediğiniz ilacı giriniz: ")

cevap = evaluator.get_response(ilac, istenilen)
print(cevap)

import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

class IlacHastalikEvaluator:
    def __init__(self, model, tokenizer, device):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device

    def generate_prompt(self, ilac: str, istenilen: str):
        default_prompt_ilac = f"<|begin_of_text|> İlaç etkileşimlerini değerlendirerek, potansiyel sağlık risklerini açıklayın.\n {ilac} ve {istenilen} birlikte kullanımı hangi kronik hastalıklara neden olur? \n"
        default_prompt_hastalik = f"<|begin_of_text|> İlaç etkileşimlerini değerlendirerek, potansiyel sağlık risklerini açıklayın.\n {ilac} hastasıyım ve {istenilen} birlikte kullanımı hangi kronik hastalıklara neden olur? \n"
        if self.is_ilac(ilac):
            return default_prompt_ilac
        elif self.is_hastalik(ilac):
            return default_prompt_hastalik
        else:
            return None

    def is_ilac(self, text: str):
        ilaç_listesi = ["aspirin", "parol", "klopidogrel", "metformin", "ağrı kesici", "grip ilacı", "kan sulandırıcı", "antibiyotik", "depresyon", "potasyum", "miğren","anestezi","sakinleştirici","barbitürat","su"]
        return any(ilac.lower() in text.lower() for ilac in ilaç_listesi)

    def is_hastalik(self, text: str):
        hastalik_listesi = ["hipertansiyon", "diyabet", "kalp", "astım", "şeker", "tansiyon", "depresyon","kanser"]
        return any(hastalik.lower() in text.lower() for hastalik in hastalik_listesi)

    def get_response(self, ilac: str, istenilen: str):
        prompt = self.generate_prompt(ilac, istenilen)

        if prompt:
            batch = self.tokenizer(prompt, return_tensors='pt').to(self.device)
            output_tokens = self.model.generate(**batch, max_new_tokens=512,eos_token_id=tokenizer.eos_token_id)
            generated_answer = self.tokenizer.decode(output_tokens[0], skip_special_tokens=True)
            # Sadece model cevabını döndürmek için prompt kısmını temizliyoruz
            return generated_answer.strip()
        else:
            return "Geçersiz giriş. Lütfen geçerli bir ilaç veya hastalık girin."

# Kullanıcı girişi ve model çıktısı
evaluator = IlacHastalikEvaluator(model, tokenizer, device)

ilac = input("Kullandığınız ilaç veya hastalığınızı giriniz: ")
istenilen = input("Kullanmak istediğiniz ilacı giriniz: ")

cevap = evaluator.get_response(ilac, istenilen)
print(cevap)